<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Wikidump 语料处理</title>
    <link href="/2022/05/22/Wikidump%20%E8%AF%AD%E6%96%99%E5%A4%84%E7%90%86/"/>
    <url>/2022/05/22/Wikidump%20%E8%AF%AD%E6%96%99%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p><a href="https://dumps.wikimedia.org/enwiki/latest/">wikidumps</a></p><p><a href="https://github.com/clab/wikipedia-parallel-titles">wikipedia-parallel-titles</a></p><p><a href="https://blog.csdn.net/qq_38796548/article/details/108318375">使用wikiextractor提取wikidumps语料</a></p><p><a href="https://blog.csdn.net/bekote/article/details/89180243">基于维基百科语料生成平行句对</a></p><p><a href="https://blog.csdn.net/weixin_40902563/article/details/89311189">基于维基百科构建平行语料库</a></p><h2 id="wikidumps语料下载"><a href="#wikidumps语料下载" class="headerlink" title="wikidumps语料下载"></a>wikidumps语料下载</h2><p>wikidumps网址为<a href="https://dumps.wikimedia.org/enwiki/latest/">en-wikidumps</a></p><h2 id="wikidumps语料的处理"><a href="#wikidumps语料的处理" class="headerlink" title="wikidumps语料的处理"></a>wikidumps语料的处理</h2><p>处理wikidumps语料可以用wikiextractor工具来提取。实现需要安装</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> wikiextractor<br></code></pre></td></tr></table></figure><p>然后可以有两种方法使用，一种是将库中的python模块用作脚本去运行</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">python -m wikiextractor<span class="hljs-selector-class">.WikiExtractor</span> enwiki-latest-pages-articles<span class="hljs-selector-class">.xml</span>.bz2<br></code></pre></td></tr></table></figure><p>或者进入安装好的wikiextractor目录运行WikiExtractor.py来处理wikidumps预料</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">python WikiExtractor<span class="hljs-selector-class">.py</span> enwiki-latest-pages-articles<span class="hljs-selector-class">.xml</span>.bz2<br></code></pre></td></tr></table></figure><p>一些常见的参数</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">1</span>.-b  文件容量  ，例如：-b <span class="hljs-number">100</span>M 当输出文件达到<span class="hljs-number">100</span>M时，自动新增文件，即可能生成多个文件<br><span class="hljs-attribute">2</span>.-o 输出文件的名称，可前面加路径，例如：-o AA_yue 或-o /extract/AA_yue，默认输出文件夹为text<br></code></pre></td></tr></table></figure><p>处理后的文件格式为</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs abnf">&lt;doc id<span class="hljs-operator">=</span><span class="hljs-string">&quot;244&quot;</span> url<span class="hljs-operator">=</span><span class="hljs-string">&quot;https://zh.wikipedia.org/wiki?curid=244&quot;</span> title<span class="hljs-operator">=</span><span class="hljs-string">&quot;史家&quot;</span>&gt;<br>史家<br><br>歷史學家也稱歷史家、史學家、史家，指以撰写历史著作为职业或对历史学的创立、发展与应用付出努力的知识分子。历史学家包括历史记录的编撰者和史料的研究者。人們研究歷史必須倚靠前人所留下的記錄。歷史學家會研究過去所發生的事件和這些事件記錄的真確性，並將他們的研究記錄下來。歷史學家的研究對象可以是某人的經歷，某城市、某地或某國家的發展。根據他們不同的研究對象，歷史可有不同的分類，例如：<br>個人歷史<br>個人歷史，是有關某人過去發生的事做研究。<br>地方歷史<br>地方歷史，是有關某城市或某地曾發生事件的研究。<br>...<br><br>&lt;/doc&gt;<br>&lt;doc id<span class="hljs-operator">=</span><span class="hljs-string">&quot;256&quot;</span> url<span class="hljs-operator">=</span><span class="hljs-string">&quot;https://zh.wikipedia.org/wiki?curid=256&quot;</span> title<span class="hljs-operator">=</span><span class="hljs-string">&quot;开放源代码&quot;</span>&gt;<br>...<br>&lt;/doc&gt;<br></code></pre></td></tr></table></figure><h2 id="wikidumps语料按照title对齐"><a href="#wikidumps语料按照title对齐" class="headerlink" title="wikidumps语料按照title对齐"></a>wikidumps语料按照title对齐</h2><h3 id="在线方式"><a href="#在线方式" class="headerlink" title="在线方式"></a>在线方式</h3><p>在线方式可以使用特定的api，如wikipedia或者wikipediaapi，具体可见</p><h3 id="离线方式"><a href="#离线方式" class="headerlink" title="离线方式"></a>离线方式</h3><p>离线方式需要我们先下载好对齐语料，然后用工具处理，然后自己编写程序获取对齐信息。</p><p>首先从wikidumps中下载对齐所需的语料，其命名格式为</p><p>*-page.sql.gz</p><p>*-langlinks.sql.gz</p><p>其中*是个前缀，一般包含语种简写和时间信息。然后使用<a href="https://github.com/clab/wikipedia-parallel-titles">wikipedia-parallel-titles</a>工具来生成标题对齐信息，该工具库中有一个<code>build-corpus.sh</code>脚本，运行此脚本即可得到标题对齐文件，运行命令为<code>./build-corpus.sh en zhwiki-latest &gt; titles.txt</code>，该命令实现的是根据zhwiki-latest得到与en的标题对齐文件titles.txt。</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>C++ 内存模型</title>
    <link href="/2022/01/30/C++%20%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"/>
    <url>/2022/01/30/C++%20%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="程序的内存分配"><a href="#程序的内存分配" class="headerlink" title="程序的内存分配"></a>程序的内存分配</h2><p><a href="https://blog.csdn.net/u013007900/article/details/79338653">https://blog.csdn.net/u013007900/article/details/79338653</a></p><p>一个由C&#x2F;C++编译的程序占用的内存分为以下几个部分：</p><ul><li>栈区（stack）— 由编译器自动分配释放，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。</li><li>堆区（heap） — 一般由程序员分配释放，若程序员不释放，程序结束时可能由OS（操作系统）回收。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。</li><li>全局区（静态区）（static）—，全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域，未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。程序结束后由系统释放。</li><li>文字常量区 —常量字符串就是放在这里的。程序结束后由系统释放。</li><li>程序代码区—存放函数体的二进制代码。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span>  a=<span class="hljs-number">0</span>;   全局初始化区    <br><br><span class="hljs-type">char</span> *p1;   全局未初始化区    <br><span class="hljs-function"><span class="hljs-type">int</span>  <span class="hljs-title">main</span><span class="hljs-params">()</span>    </span><br><span class="hljs-function"></span>&#123;    <br>  <span class="hljs-type">int</span>  b; <span class="hljs-comment">//栈    </span><br>  <span class="hljs-type">char</span>  s[]=<span class="hljs-string">&quot;abc&quot;</span>; <span class="hljs-comment">//栈    </span><br>  <span class="hljs-type">char</span>  *p2; <span class="hljs-comment">//栈    </span><br>  <span class="hljs-type">char</span>  *p3=<span class="hljs-string">&quot;123456&quot;</span>; <span class="hljs-comment">//123456/0在常量区，p3在栈上。    </span><br><br>  <span class="hljs-type">static</span> <span class="hljs-type">int</span> c =<span class="hljs-number">0</span>；<span class="hljs-comment">//全局（静态）初始化区    </span><br>  p1 =  (<span class="hljs-type">char</span>  *)<span class="hljs-built_in">malloc</span>(<span class="hljs-number">10</span>);  <span class="hljs-comment">//分配得来得10和20字节的区域就在堆区</span><br>  p2  = (<span class="hljs-type">char</span>  *)<span class="hljs-built_in">malloc</span>(<span class="hljs-number">20</span>);       <br>  <span class="hljs-built_in">strcpy</span>(p3,<span class="hljs-string">&quot;123456&quot;</span>); <span class="hljs-comment">//123456/0放在常量区，编译器可能会将它与p3所指向的&quot;123456&quot;  优化成一个地方。    </span><br>&#125;    <br><br></code></pre></td></tr></table></figure><h2 id="头文件"><a href="#头文件" class="headerlink" title="头文件"></a>头文件</h2><p>不要将函数定义或变量声明放到头文件中，如果同一程序的其他两个文件包含了该头文件，此时同一个程序将包含同一个函数的两个定义，即多重声明问题，这会导致错误。头文件常包含以下内容：</p><ul><li>函数原型(函数声明)</li><li>宏定义；#define或const定义的符号常量。</li><li>结构声明</li><li>类声明</li><li>模板声明</li><li>内联函数</li></ul><p>源代码文件用于具体实现，即头文件中所声明的函数或类的具体实现。</p><p>注意，只需将源代码文件加入到项目中，而不用加入头文件。这是因为#include指令管理头文件。另外，不要使用#include来包含源代码文件，这样做将导致多重声明。</p><p><a href="https://blog.csdn.net/nei504293736/article/details/90200066">https://blog.csdn.net/nei504293736/article/details/90200066</a></p><p>模板类的分离问题：</p><ul><li>声明和实现都写到.h文件中</li><li>或者，声明写到.h中，实现写到.cpp中，同时在.cpp文件中声明具体的模板类(显式实例化)</li></ul><p><a href="https://www.jianshu.com/p/bd2e05aabf7a">https://www.jianshu.com/p/bd2e05aabf7a</a></p><p><a href="https://stackoverflow.com/questions/495021/why-can-templates-only-be-implemented-in-the-header-file">https://stackoverflow.com/questions/495021/why-can-templates-only-be-implemented-in-the-header-file</a></p><h2 id="存储持续性、作用域和链接"><a href="#存储持续性、作用域和链接" class="headerlink" title="存储持续性、作用域和链接"></a>存储持续性、作用域和链接</h2><p>有4中内存方案：</p><ul><li>自动存储持续性，在函数定义中声明的 变量（包括函数参数）的存储持续性是自动的。它们在程序开始执行其所属的函数或代码块时被创建，在执行完函数或代码块时，它们使用的内存被释放。</li><li>静态存储持续性，在函数定义外定义的变量和使用关键字static定义的变量的存储持续性都为静态。它们在程序整个运行过程中都存在。</li><li>线程存储持续性，对于多线程来说，如果变量是使用关键字thread_local声明的，则其生命周期与所属的线程一样长。</li><li>动态存储持续性，用new运算符分配的内存将一直存在，直到使用delete运算符将其释放或程序结束为止。这种内存的存储持续性为动态，有时被称为自由存储（free store）或堆（heap）。</li></ul><p>作用域(scope)描述了名称在文件的多大范围内可见。可分为局部和全局，作用域为局部的变量只在定义它的代码块中可用，作用域为全局的变量在定义位置到文件结尾之间都可用。</p><p>链接性(linkage)描述了名称如何在不同单元间共享。链接性为外部的名称可在文件间共享，链接性为内部的名称只能由一个文件中的函数共享。</p><h2 id="静态持续变量与static"><a href="#静态持续变量与static" class="headerlink" title="静态持续变量与static"></a>静态持续变量与static</h2><p>程序不会使用栈或者其他来管理静态变量，而是编译器会分配固定的内存块来存储静态变量，这样这些变量在整个程序执行期间一直存在。另外如果没有显式地初始化静态变量，编译器将把它设置为默认值。</p><p>C++为静态存储持续性变量提供了3种链接性：</p><ul><li>外部链接性，可在其他文件中访问</li><li>内部链接性，只能在当前文件中访问</li><li>无链接性，只能在当前函数或代码块中访问</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-type">int</span> global = <span class="hljs-number">10</span>;  <span class="hljs-comment">//静态持续，外部链接</span><br><span class="hljs-type">static</span> <span class="hljs-type">int</span> one_file = <span class="hljs-number">50</span>;  <span class="hljs-comment">//静态持续，内部链接</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">fun1</span><span class="hljs-params">(<span class="hljs-type">int</span> n)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-type">static</span> <span class="hljs-type">int</span> count = <span class="hljs-number">0</span>;  <span class="hljs-comment">//静态持续，无链接性</span><br>    <span class="hljs-type">int</span> llama = <span class="hljs-number">0</span>;<br>&#125;<br><span class="hljs-comment">/*</span><br><span class="hljs-comment">函数内的static限定表明该变量会一直驻留在内存中，而函数内的普通变量会随着函数结束而生命周期结束</span><br><span class="hljs-comment">函数外的static限定表明该变量是内部链接的，而函数外的普通变量是外部链接的</span><br><span class="hljs-comment">*/</span><br></code></pre></td></tr></table></figure><p>所有静态持续变量在整个程序执行期间都存在。fun1()中声明的变量count的作用域是局部且没有链接性，但是该变量会一直存留在内存中；global和one_file的作用域都为整个文件，而one_file的链接性为内部只能在本文件中使用它，而global的链接性为外部，因此可在其他程序文件中使用它。</p><h2 id="extern与static"><a href="#extern与static" class="headerlink" title="extern与static"></a>extern与static</h2><p>链接性为外部的变量通常简称为外部变量，它们的存储持续性为静态，作用域为整个文件。外部变量是在函数外部定义的，其也可被称为全局变量。</p><p>**单定义规则(One Definition Rule, ODR)**：变量只能由一次定义。C++提供了两种变量声明。一种是定义声明(defining declaration)或简称为定义(definition)，它给变量分配存储空间；另一种声明是引用声明(referencing declaration)或简称为声明(declaration)，它不给变量分配存储空间，因为它引用已有的变量。引用声明使用关键字extern，且不进行初始化。</p><p>引用声明使用关键字extern，且不进行初始化；否则，声明为定义，导致分配存储空间。如果要在多个文件中使用外部变量，只需在一个文件中包含该变量的定义(当定义规则)，但在使用该变量的其他所有文件中，都必须使用关键字extern声明它。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//file1.cpp</span><br><span class="hljs-type">int</span> a;  <span class="hljs-comment">//定义，默认为0</span><br><span class="hljs-type">static</span> <span class="hljs-type">int</span> b = <span class="hljs-number">4</span>;  <span class="hljs-comment">// 定义，因为初始化了</span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> c = <span class="hljs-number">5</span>;<br><span class="hljs-keyword">extern</span> <span class="hljs-type">const</span> <span class="hljs-type">int</span> d = <span class="hljs-number">6</span>;<br><span class="hljs-keyword">extern</span> <span class="hljs-type">int</span> e = <span class="hljs-number">7</span>;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">fun1</span><span class="hljs-params">(<span class="hljs-type">int</span> m, <span class="hljs-type">int</span> n)</span> </span>&#123;<br>    <span class="hljs-comment">//...</span><br>&#125;<br><br><span class="hljs-comment">//file2.cpp</span><br><span class="hljs-keyword">extern</span> <span class="hljs-type">int</span> a, d, e;  <span class="hljs-comment">//声明，因为有extern且未初始化</span><br><span class="hljs-type">int</span> b, c;  <span class="hljs-comment">// static, const的链接性均为内部</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">fun1</span><span class="hljs-params">(<span class="hljs-type">int</span>, <span class="hljs-type">int</span>)</span></span>;  <span class="hljs-comment">//函数声明的extern可省略</span><br></code></pre></td></tr></table></figure><h2 id="其他说明符和限定符"><a href="#其他说明符和限定符" class="headerlink" title="其他说明符和限定符"></a>其他说明符和限定符</h2><ul><li><p>const</p><p>声明为const的变量必须在声明的时候就初始化，并且一旦初始化后，其值就不可改变，且其声明一般是放在头文件中。const变量的链接性为内部。不过可以在定义时使用extern修饰来覆盖默认的内部链接性，将该变量的链接性为外部，</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">extern</span> <span class="hljs-type">const</span> <span class="hljs-type">int</span> states = <span class="hljs-number">50</span>; <span class="hljs-comment">// definition with external linkage</span><br></code></pre></td></tr></table></figure><p>此时，必须在所有使用该常量的文件中使用extern关键字来声明它，这与常规外部变量不同，定义常规外部变量时，不必使用extern关键字，但在使用该变量的其他文件中必须使用extern。</p></li><li><p>volatile</p><p>提醒编译器它后面所定义的变量随时都有可能改变，因此编译后的程序每次需要存储或读取这个变量的时候，都会直接从变量地址中读取数据。如果没有volatile关键字，则编译器可能优化读取和存储，可能暂时使用寄存器中的值，如果这个变量由别的程序更新了的话，将出现不一致的现象。</p></li><li><p>mutable</p><p>mutable修饰的变量表明其无论如何是可改变的。比如一个const对象，其成员变量不可变，此时若声明某个成员变量是mutable的，则它依旧可以通过该const对象来改变该成员变量的值。</p></li></ul><h2 id="函数和链接性"><a href="#函数和链接性" class="headerlink" title="函数和链接性"></a>函数和链接性</h2><p>所有函数的存储持续性都为静态的，即在整个程序执行期间都存在。默认情况下，函数的链接性为外部，即可以在多个文件中共享，但是在另一个文件中可以不使用extern关键字来声明函数原型，而可以直接声明函数原型即可。如果使用static关键字修饰函数，则其链接性将变为内部，此时其只能在一个文件中使用而不能在其他文件中声明和使用，必须在函数原型和函数定义中都使用该关键字，此时如果外部有同名函数，该静态函数将覆盖外部同名函数，就同局部变量覆盖全局变量一样。</p>]]></content>
    
    
    <categories>
      
      <category>C++</category>
      
    </categories>
    
    
    <tags>
      
      <tag>C++</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PPL 计算</title>
    <link href="/2021/09/28/PPL%20%E8%AE%A1%E7%AE%97/"/>
    <url>/2021/09/28/PPL%20%E8%AE%A1%E7%AE%97/</url>
    
    <content type="html"><![CDATA[<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p><a href="https://github.com/xu-song/bert-as-language-model">https://github.com/xu-song/bert-as-language-model</a></p><p><a href="https://stackoverflow.com/questions/63030692/how-do-i-use-bertformaskedlm-or-bertmodel-to-calculate-perplexity-of-a-sentence">https://stackoverflow.com/questions/63030692/how-do-i-use-bertformaskedlm-or-bertmodel-to-calculate-perplexity-of-a-sentence</a></p><p><a href="https://github.com/ymcui/Chinese-BERT-wwm">https://github.com/ymcui/Chinese-BERT-wwm</a></p><p>对于给定的sentence，按顺序依次mask掉一个token，并计算所预测单词的nll loss，将所有的token的loss求和再取平均，最后取以自然数为底的次方即为该句话的PPL。</p><p>测试写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, BertForMaskedLM<br><span class="hljs-comment"># Load pre-trained model (weights)</span><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    model = BertForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;hfl/chinese-bert-wwm-ext&#x27;</span>)<br>    model.<span class="hljs-built_in">eval</span>()<br>    <span class="hljs-comment"># Load pre-trained model tokenizer (vocabulary)</span><br>    tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;hfl/chinese-bert-wwm-ext&#x27;</span>)<br>    sentence = <span class="hljs-string">&quot;我不会忘记和你一起奋斗的时光。&quot;</span><br>    tokenize_input = tokenizer.tokenize(sentence)<br>    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])<br>    sen_len = <span class="hljs-built_in">len</span>(tokenize_input)<br>    sentence_loss = <span class="hljs-number">0.</span><br><br>    <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tokenize_input):<br>        <span class="hljs-comment"># add mask to i-th character of the sentence</span><br>        tokenize_input[i] = <span class="hljs-string">&#x27;[MASK]&#x27;</span><br>        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])<br><br>        output = model(mask_input)<br><br>        prediction_scores = output[<span class="hljs-number">0</span>]<br>        softmax = nn.Softmax(dim=<span class="hljs-number">0</span>)<br>        ps = softmax(prediction_scores[<span class="hljs-number">0</span>, i]).log()<br>        word_loss = ps[tensor_input[<span class="hljs-number">0</span>, i]]<br>        sentence_loss += word_loss.item()<br><br>        tokenize_input[i] = word<br>    ppl = np.exp(-sentence_loss/sen_len)<br>    <span class="hljs-built_in">print</span>(ppl)<br></code></pre></td></tr></table></figure><p>tensor思维的写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">score</span>(<span class="hljs-params">model, tokenizer, sentence,  mask_token_id=<span class="hljs-number">103</span></span>):<br>  tensor_input = tokenizer.encode(sentence, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>  repeat_input = tensor_input.repeat(tensor_input.size(-<span class="hljs-number">1</span>)-<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>  mask = torch.ones(tensor_input.size(-<span class="hljs-number">1</span>) - <span class="hljs-number">1</span>).diag(<span class="hljs-number">1</span>)[:-<span class="hljs-number">2</span>]<br>  masked_input = repeat_input.masked_fill(mask == <span class="hljs-number">1</span>, <span class="hljs-number">103</span>)<br>  labels = repeat_input.masked_fill( masked_input != <span class="hljs-number">103</span>, -<span class="hljs-number">100</span>)<br>  loss,_ = model(masked_input, masked_lm_labels=labels)<br>  result = np.exp(loss.item())<br>  <span class="hljs-keyword">return</span> result<br><br>s = score(model, tokenizer, <span class="hljs-string">&#x27;我不会忘记和你一起奋斗的时光。&#x27;</span>)<br><span class="hljs-built_in">print</span>(s)<br></code></pre></td></tr></table></figure><h2 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h2><p><a href="https://github.com/Morizeyao/GPT2-Chinese">https://github.com/Morizeyao/GPT2-Chinese</a></p><p>官方的gpt-2不支持中文，且是BPE分词方式。对于中文，有NLPer训练出了中文的gpt-2模型，且分词采用的是bert tokenizer的分词方式。</p><p>对于给定的sentence，若其长度为n，首先将其向左偏移一位作为label，将其去除末位作为input，将gpt-2的输出与label求cross entroy loss，再求以自然数为底的次方即为该句话的PPL。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, GPT2LMHeadModel<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> CrossEntropyLoss<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cal_ppl_bygpt2</span>():<br>    sens = [<span class="hljs-string">&quot;今天是个好日子。&quot;</span>, <span class="hljs-string">&quot;天今子日。个是好&quot;</span>, <span class="hljs-string">&quot;这个婴儿有900000克呢。&quot;</span>, <span class="hljs-string">&quot;我不会忘记和你一起奋斗的时光。&quot;</span>,<br>            <span class="hljs-string">&quot;我不会记忘和你一起奋斗的时光。&quot;</span>, <span class="hljs-string">&quot;会我记忘和你斗起一奋的时光。&quot;</span>]<br>    tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;uer/gpt2-chinese-cluecorpussmall&quot;</span>)<br>    model = GPT2LMHeadModel.from_pretrained(<span class="hljs-string">&quot;uer/gpt2-chinese-cluecorpussmall&quot;</span>)<br>    inputs = tokenizer(sens, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>, max_length=<span class="hljs-number">50</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br>    bs, sl = inputs[<span class="hljs-string">&#x27;input_ids&#x27;</span>].size()<br>    outputs = model(**inputs, labels=inputs[<span class="hljs-string">&#x27;input_ids&#x27;</span>])<br>    logits = outputs[<span class="hljs-number">1</span>]<br>    <span class="hljs-comment"># Shift so that tokens &lt; n predict n</span><br>    shift_logits = logits[:, :-<span class="hljs-number">1</span>, :].contiguous()<br>    shift_labels = inputs[<span class="hljs-string">&#x27;input_ids&#x27;</span>][:, <span class="hljs-number">1</span>:].contiguous()<br>    shift_attentions = inputs[<span class="hljs-string">&#x27;attention_mask&#x27;</span>][:, <span class="hljs-number">1</span>:].contiguous()<br>    <span class="hljs-comment"># Flatten the tokens</span><br>    loss_fct = CrossEntropyLoss(ignore_index=<span class="hljs-number">0</span>, reduction=<span class="hljs-string">&quot;none&quot;</span>)<br>    loss = loss_fct(shift_logits.view(-<span class="hljs-number">1</span>, shift_logits.size(-<span class="hljs-number">1</span>)), shift_labels.view(-<span class="hljs-number">1</span>)).detach().reshape(bs, -<span class="hljs-number">1</span>)<br>    meanloss = loss.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>) / shift_attentions.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>)<br>    ppl = torch.exp(meanloss).numpy().tolist()<br>    <span class="hljs-keyword">return</span> ppl<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    cal_ppl_bygpt2()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
